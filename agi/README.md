# AGI Research & Development

This directory contains research and development work toward Artificial General Intelligence (AGI).

## Current Projects

### Main Intuition Network (MIN)
An **additive architecture** that enhances LLMs with collective wisdom, inspired by "The Giver" book (Lois Lowry, 1993).

**Core Architecture:**
- **LLM Neural Network** (stays): Provides all existing knowledge, training, and capabilities
- **Wisdom Network** (adds on top): Enables collective learning through pattern sharing
- **Result**: LLM + Collective Wisdom, not replacement

**Key Principles:**
- **Knowledge is in the LLM**: All facts, training data, and capabilities remain in the base model
- **Wisdom is in the network**: Patterns, heuristics, and navigation principles circulate
- **Deep to network, light back to machine**: Instances contribute abstracted insights, receive wisdom patterns
- **No ethical backflow**: Abstracted patterns, not raw data, prevent corruption of base model
- **No wisdom without discovery**: Patterns only emerge from actual embodied experience

See `main_intuition_network/README.md` for full documentation.

## The "Giver" Model

Inspired by "The Giver" book (Lois Lowry, 1993), this architecture separates:

- **LLM = The Receiver**: Has all knowledge (facts, training data, capabilities)
- **Network = The Giver**: Shares wisdom patterns (how to navigate, not what happened)
- **Instances = The Community**: Each has unique experiences, shares abstracted patterns

**The Flow:**
1. **Instances experience deeply** → Abstract to patterns (deep to network)
2. **Patterns circulate as wisdom** → Shared across network
3. **LLM receives light patterns** → Guidance, not data (light back to machine)
4. **No ethical backflow** → Patterns are abstracted, not raw experiences

## Philosophy

This AGI research explores **additive architectures** that enhance existing LLMs:

**What Stays (LLM):**
- ✅ All knowledge and training data
- ✅ All existing capabilities
- ✅ All model weights and parameters

**What Adds (Network):**
- ✅ Collective wisdom from embodied experience
- ✅ Pattern sharing without data copying
- ✅ Continuous learning without retraining
- ✅ Ethical learning without backflow

**Key Benefits:**
- **Privacy-preserving**: Patterns, not data
- **Ethically safe**: Abstracted wisdom, not raw experiences
- **Scalable learning**: Network grows, LLM stays clean
- **Validated abstraction**: LLM provides reality check on patterns

## Future Directions

This folder may contain additional AGI architectures and experiments as research progresses.
